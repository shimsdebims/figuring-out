{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ± CropGuard - CCMT Disease Detection Model Training\n",
    "\n",
    "**Dataset:** CCMT (Cashew, Cassava, Maize, Tomato)  \n",
    "**Model:** MobileNetV2 (Lightweight, ~14MB)  \n",
    "**Classes:** 22 diseases across 4 crops\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Dataset Structure:\n",
    "- **Cashew** (5 classes): anthracnose, gumosis, healthy, leaf miner, red rust\n",
    "- **Cassava** (5 classes): bacterial blight, brown spot, green mite, healthy, mosaic\n",
    "- **Maize** (7 classes): fall armyworm, grasshopper, healthy, leaf beetle, leaf blight, leaf spot, streak virus\n",
    "- **Tomato** (5 classes): healthy, leaf blight, leaf curl, septoria leaf spot, verticillium wilt\n",
    "\n",
    "**Total: 22 disease classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 1: Setup & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", len(tf.config.list_physical_devices('GPU')) > 0)\n",
    "print(\"Num GPUs:\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Install additional packages if needed\n",
    "!pip install -q pillow matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Step 2: Upload Dataset to Colab\n",
    "\n",
    "**Option A:** Upload ZIP file  \n",
    "**Option B:** Mount Google Drive (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload dataset ZIP file\n",
    "# Uncomment these lines if you uploaded a ZIP:\n",
    "# !unzip -q \"/content/CCMT_Dataset.zip\" -d /content/dataset\n",
    "# DATA_DIR = \"/content/dataset/CCMT Dataset\"\n",
    "\n",
    "# Option B: Mount Google Drive (RECOMMENDED)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update this path to where your dataset is in Google Drive\n",
    "DATA_DIR = \"/content/drive/MyDrive/CCMT Dataset\"  # <-- CHANGE THIS PATH\n",
    "\n",
    "# Verify dataset exists\n",
    "import os\n",
    "print(\"\\nðŸ“ Dataset contents:\")\n",
    "!ls \"{DATA_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 3: Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Count images per class\n",
    "dataset_stats = {}\n",
    "total_images = 0\n",
    "\n",
    "for crop in os.listdir(DATA_DIR):\n",
    "    crop_path = os.path.join(DATA_DIR, crop)\n",
    "    if not os.path.isdir(crop_path):\n",
    "        continue\n",
    "    \n",
    "    dataset_stats[crop] = {}\n",
    "    for disease in os.listdir(crop_path):\n",
    "        disease_path = os.path.join(crop_path, disease)\n",
    "        if not os.path.isdir(disease_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(disease_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        count = len(images)\n",
    "        dataset_stats[crop][disease] = count\n",
    "        total_images += count\n",
    "\n",
    "# Display stats\n",
    "print(\"\\nðŸ“Š Dataset Statistics:\\n\")\n",
    "for crop, diseases in dataset_stats.items():\n",
    "    print(f\"\\nðŸŒ¾ {crop.upper()}:\")\n",
    "    for disease, count in diseases.items():\n",
    "        print(f\"   {disease}: {count} images\")\n",
    "    print(f\"   TOTAL: {sum(diseases.values())} images\")\n",
    "\n",
    "print(f\"\\n\\nðŸŽ¯ TOTAL IMAGES: {total_images}\")\n",
    "print(f\"ðŸ“¦ Total Classes: {sum(len(d) for d in dataset_stats.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Prepare data for plotting\n",
    "all_classes = []\n",
    "all_counts = []\n",
    "colors_map = {'Cashew': '#FF6B6B', 'Cassava': '#4ECDC4', 'Maize': '#FFE66D', 'Tomato': '#FF4757'}\n",
    "colors = []\n",
    "\n",
    "for crop, diseases in dataset_stats.items():\n",
    "    for disease, count in diseases.items():\n",
    "        all_classes.append(f\"{crop}_{disease}\")\n",
    "        all_counts.append(count)\n",
    "        colors.append(colors_map.get(crop, '#95A5A6'))\n",
    "\n",
    "plt.bar(range(len(all_classes)), all_counts, color=colors, alpha=0.8, edgecolor='black')\n",
    "plt.xticks(range(len(all_classes)), all_classes, rotation=90, ha='right')\n",
    "plt.xlabel('Disease Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Images', fontsize=12, fontweight='bold')\n",
    "plt.title('ðŸŒ± CCMT Dataset - Class Distribution', fontsize=16, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Dataset exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ–¼ï¸ Step 4: Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each crop\n",
    "fig, axes = plt.subplots(4, 5, figsize=(18, 14))\n",
    "fig.suptitle('ðŸŒ± Sample Images from CCMT Dataset', fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "crop_idx = 0\n",
    "for crop in ['Cashew', 'Cassava', 'Maize', 'Tomato']:\n",
    "    crop_path = os.path.join(DATA_DIR, crop)\n",
    "    diseases = [d for d in os.listdir(crop_path) if os.path.isdir(os.path.join(crop_path, d))]\n",
    "    \n",
    "    for col_idx in range(min(5, len(diseases))):\n",
    "        disease = diseases[col_idx]\n",
    "        disease_path = os.path.join(crop_path, disease)\n",
    "        images = [f for f in os.listdir(disease_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if images:\n",
    "            img_path = os.path.join(disease_path, images[0])\n",
    "            img = Image.open(img_path)\n",
    "            axes[crop_idx, col_idx].imshow(img)\n",
    "            axes[crop_idx, col_idx].set_title(f\"{crop}\\n{disease}\", fontsize=10, fontweight='bold')\n",
    "            axes[crop_idx, col_idx].axis('off')\n",
    "    \n",
    "    crop_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¨ Step 5: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Image parameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "# No augmentation for validation\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\nâœ… Data prepared!\")\n",
    "print(f\"ðŸ“Š Training samples: {train_generator.samples}\")\n",
    "print(f\"ðŸ“Š Validation samples: {validation_generator.samples}\")\n",
    "print(f\"ðŸŽ¯ Number of classes: {num_classes}\")\n",
    "print(f\"\\nðŸ“ Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Step 6: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load pre-trained MobileNetV2 (without top layer)\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze base model initially\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Model built successfully!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 7: Train Model (Phase 1 - Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import time\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'ccmt_model_phase1.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Phase 1\n",
    "print(\"\\nðŸš€ Starting Phase 1: Transfer Learning (Base frozen)\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "EPOCHS_PHASE1 = 20\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint, reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase1_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Phase 1 complete in {phase1_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”“ Step 8: Fine-Tuning (Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last layers of base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except the last 30\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# New checkpoint for phase 2\n",
    "checkpoint_phase2 = ModelCheckpoint(\n",
    "    'ccmt_model_final.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Starting Phase 2: Fine-Tuning (Last 30 layers unfrozen)\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "EPOCHS_PHASE2 = 15\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint_phase2, reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase2_time = time.time() - start_time\n",
    "total_training_time = phase1_time + phase2_time\n",
    "\n",
    "print(f\"\\nâœ… Phase 2 complete in {phase2_time/60:.1f} minutes\")\n",
    "print(f\"\\nðŸŽ¯ Total training time: {total_training_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 9: Training Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories from both phases\n",
    "history_combined = {\n",
    "    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n",
    "    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],\n",
    "    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n",
    "    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
    "}\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('ðŸŽ¯ Training Performance - CCMT CropGuard Model', fontsize=18, fontweight='bold', y=1.02)\n",
    "\n",
    "# Accuracy plot\n",
    "epochs_range = range(1, len(history_combined['accuracy']) + 1)\n",
    "ax1.plot(epochs_range, history_combined['accuracy'], 'o-', color='#2ecc71', linewidth=2, label='Training Accuracy', markersize=4)\n",
    "ax1.plot(epochs_range, history_combined['val_accuracy'], 's-', color='#3498db', linewidth=2, label='Validation Accuracy', markersize=4)\n",
    "ax1.axvline(x=EPOCHS_PHASE1, color='red', linestyle='--', linewidth=2, label='Fine-Tuning Start', alpha=0.7)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('ðŸ“ˆ Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='lower right', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(epochs_range, history_combined['loss'], 'o-', color='#e74c3c', linewidth=2, label='Training Loss', markersize=4)\n",
    "ax2.plot(epochs_range, history_combined['val_loss'], 's-', color='#f39c12', linewidth=2, label='Validation Loss', markersize=4)\n",
    "ax2.axvline(x=EPOCHS_PHASE1, color='red', linestyle='--', linewidth=2, label='Fine-Tuning Start', alpha=0.7)\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('ðŸ“‰ Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history_combined['accuracy'][-1]\n",
    "final_val_acc = history_combined['val_accuracy'][-1]\n",
    "final_train_loss = history_combined['loss'][-1]\n",
    "final_val_loss = history_combined['val_loss'][-1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š FINAL TRAINING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… Training Accuracy: {final_train_acc*100:.2f}%\")\n",
    "print(f\"âœ… Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
    "print(f\"ðŸ“‰ Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"ðŸ“‰ Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"â±ï¸ Total Training Time: {total_training_time/60:.1f} minutes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 10: Evaluate Model & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions\n",
    "print(\"\\nðŸ” Generating predictions on validation set...\")\n",
    "validation_generator.reset()\n",
    "predictions = model.predict(validation_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = validation_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“‹ CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(true_classes, predicted_classes, target_names=class_names, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(18, 16))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='YlOrRd',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "plt.title('ðŸ”¥ Confusion Matrix - CCMT CropGuard Model', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 11: Per-Class Accuracy Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "class_accuracy_dict = dict(zip(class_names, per_class_accuracy))\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_classes = sorted(class_accuracy_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_names = [x[0] for x in sorted_classes]\n",
    "sorted_accuracies = [x[1]*100 for x in sorted_classes]\n",
    "\n",
    "# Color code by accuracy\n",
    "colors = ['#2ecc71' if acc >= 90 else '#f39c12' if acc >= 75 else '#e74c3c' for acc in sorted_accuracies]\n",
    "\n",
    "# Plot per-class accuracy\n",
    "plt.figure(figsize=(16, 10))\n",
    "bars = plt.barh(sorted_names, sorted_accuracies, color=colors, edgecolor='black', linewidth=1.2, alpha=0.85)\n",
    "plt.xlabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Disease Class', fontsize=14, fontweight='bold')\n",
    "plt.title('ðŸŽ¯ Per-Class Accuracy - CCMT CropGuard Model', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlim([0, 105])\n",
    "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, acc) in enumerate(zip(bars, sorted_accuracies)):\n",
    "    plt.text(acc + 1, i, f'{acc:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', label='Excellent (â‰¥90%)'),\n",
    "    Patch(facecolor='#f39c12', label='Good (75-89%)'),\n",
    "    Patch(facecolor='#e74c3c', label='Needs Improvement (<75%)')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print worst performing classes\n",
    "print(\"\\nâš ï¸ Classes needing improvement (<80% accuracy):\")\n",
    "for name, acc in sorted_classes:\n",
    "    if acc < 0.8:\n",
    "        print(f\"   {name}: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 12: Save Model & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save final model in multiple formats\n",
    "print(\"\\nðŸ’¾ Saving model in multiple formats...\\n\")\n",
    "\n",
    "# 1. HDF5 format (Keras)\n",
    "model.save('ccmt_cropguard_final.h5')\n",
    "print(\"âœ… Saved: ccmt_cropguard_final.h5\")\n",
    "\n",
    "# 2. SavedModel format (TensorFlow)\n",
    "model.save('ccmt_cropguard_savedmodel', save_format='tf')\n",
    "print(\"âœ… Saved: ccmt_cropguard_savedmodel/\")\n",
    "\n",
    "# 3. TFLite format (for mobile/edge)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "with open('ccmt_cropguard_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"âœ… Saved: ccmt_cropguard_model.tflite (optimized)\")\n",
    "\n",
    "# Save class names\n",
    "class_info = {\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': class_names,\n",
    "    'class_indices': train_generator.class_indices\n",
    "}\n",
    "with open('class_info.json', 'w') as f:\n",
    "    json.dump(class_info, f, indent=2)\n",
    "print(\"âœ… Saved: class_info.json\")\n",
    "\n",
    "# Check file sizes\n",
    "print(\"\\nðŸ“¦ Model File Sizes:\")\n",
    "!ls -lh ccmt_cropguard_final.h5\n",
    "!ls -lh ccmt_cropguard_model.tflite\n",
    "\n",
    "# Calculate model size\n",
    "import os\n",
    "h5_size = os.path.getsize('ccmt_cropguard_final.h5') / (1024*1024)\n",
    "tflite_size = os.path.getsize('ccmt_cropguard_model.tflite') / (1024*1024)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Sizes:\")\n",
    "print(f\"   H5 Format: {h5_size:.2f} MB\")\n",
    "print(f\"   TFLite Format: {tflite_size:.2f} MB (optimized)\")\n",
    "print(f\"   Size Reduction: {(1 - tflite_size/h5_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Step 13: Final Summary & Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create beautiful summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŒ± CCMT CROPGUARD MODEL - TRAINING COMPLETE ðŸŒ±\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š DATASET INFORMATION:\")\n",
    "print(f\"   Total Images: {total_images:,}\")\n",
    "print(f\"   Training Samples: {train_generator.samples:,}\")\n",
    "print(f\"   Validation Samples: {validation_generator.samples:,}\")\n",
    "print(f\"   Number of Classes: {num_classes}\")\n",
    "print(f\"   Crops: Cashew, Cassava, Maize, Tomato\")\n",
    "\n",
    "print(\"\\nðŸ§  MODEL ARCHITECTURE:\")\n",
    "print(f\"   Base Model: MobileNetV2 (ImageNet pretrained)\")\n",
    "print(f\"   Input Size: {IMG_SIZE}x{IMG_SIZE}x3\")\n",
    "print(f\"   Total Parameters: {model.count_params():,}\")\n",
    "print(f\"   Model Size (H5): {h5_size:.2f} MB\")\n",
    "print(f\"   Model Size (TFLite): {tflite_size:.2f} MB\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ PERFORMANCE METRICS:\")\n",
    "print(f\"   Final Training Accuracy: {final_train_acc*100:.2f}%\")\n",
    "print(f\"   Final Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
    "print(f\"   Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"   Validation Loss: {final_val_loss:.4f}\")\n",
    "print(f\"   Total Epochs: {len(history_combined['accuracy'])}\")\n",
    "print(f\"   Training Time: {total_training_time/60:.1f} minutes\")\n",
    "\n",
    "print(\"\\nðŸ“ SAVED FILES:\")\n",
    "print(\"   âœ… ccmt_cropguard_final.h5 - Full Keras model\")\n",
    "print(\"   âœ… ccmt_cropguard_model.tflite - Optimized for mobile/edge\")\n",
    "print(\"   âœ… ccmt_cropguard_savedmodel/ - TensorFlow SavedModel format\")\n",
    "print(\"   âœ… class_info.json - Class names and indices\")\n",
    "print(\"   âœ… training_history.png - Training curves\")\n",
    "print(\"   âœ… confusion_matrix.png - Confusion matrix heatmap\")\n",
    "print(\"   âœ… per_class_accuracy.png - Per-class performance\")\n",
    "\n",
    "print(\"\\nðŸš€ NEXT STEPS:\")\n",
    "print(\"   1. Download all model files to your computer\")\n",
    "print(\"   2. Upload ccmt_cropguard_final.h5 to Google Drive\")\n",
    "print(\"   3. Update your CropGuard app to load this model\")\n",
    "print(\"   4. Test with real crop images\")\n",
    "print(\"   5. Deploy to Streamlit Cloud\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ CONGRATULATIONS! Your model is ready to use! ðŸŽ‰\".center(70))\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 14: Download Files to Your Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to download files from Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ðŸ“¥ Preparing files for download...\\n\")\n",
    "\n",
    "# Download main model file\n",
    "print(\"Downloading: ccmt_cropguard_final.h5\")\n",
    "files.download('ccmt_cropguard_final.h5')\n",
    "\n",
    "# Download TFLite model\n",
    "print(\"Downloading: ccmt_cropguard_model.tflite\")\n",
    "files.download('ccmt_cropguard_model.tflite')\n",
    "\n",
    "# Download class info\n",
    "print(\"Downloading: class_info.json\")\n",
    "files.download('class_info.json')\n",
    "\n",
    "# Download visualizations\n",
    "print(\"Downloading: training_history.png\")\n",
    "files.download('training_history.png')\n",
    "\n",
    "print(\"Downloading: confusion_matrix.png\")\n",
    "files.download('confusion_matrix.png')\n",
    "\n",
    "print(\"Downloading: per_class_accuracy.png\")\n",
    "files.download('per_class_accuracy.png')\n",
    "\n",
    "print(\"\\nâœ… All files downloaded!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
